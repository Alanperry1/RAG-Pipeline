{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cacad3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from typing import List,Dict,Any,Tuple\n",
    "from chromadb.config import Settings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pathlib import Path\n",
    "import os\n",
    "import numpy as np\n",
    "import uuid\n",
    "import chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1c5d1d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Document Object\n",
    "doc=Document(\n",
    "    page_content=\"this is the main context I am using to create RAG\",\n",
    "    metadata={\n",
    "        \"source\":\"example.txt\",\n",
    "        \"pages\":1,\n",
    "        \"author\":\"Krish Naik\",\n",
    "        \"date_created\":\"2025-11-05\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7df82186",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a txt file\n",
    "os.makedirs(\"../data/text_files\",exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b4dfc8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample files created\n"
     ]
    }
   ],
   "source": [
    "sample_texts=sample_texts={\n",
    "    \"../data/text_files/python_intro.txt\":\"\"\"Python Programming Introduction\n",
    "\n",
    "Python is a high-level, interpreted programming language known for its simplicity and readability.\n",
    "Created by Guido van Rossum and first released in 1991, Python has become one of the most popular\n",
    "programming languages in the world.\n",
    "\n",
    "Key Features:\n",
    "- Easy to learn and use\n",
    "- Extensive standard library\n",
    "- Cross-platform compatibility\n",
    "- Strong community support\n",
    "\n",
    "Python is widely used in web development, data science, artificial intelligence, and automation.\"\"\",\n",
    "    \n",
    "    \"../data/text_files/machine_learning.txt\": \"\"\"Machine Learning Basics\n",
    "\n",
    "Machine learning is a subset of artificial intelligence that enables systems to learn and improve\n",
    "from experience without being explicitly programmed. It focuses on developing computer programs\n",
    "that can access data and use it to learn for themselves.\n",
    "\n",
    "Types of Machine Learning:\n",
    "1. Supervised Learning: Learning with labeled data\n",
    "2. Unsupervised Learning: Finding patterns in unlabeled data\n",
    "3. Reinforcement Learning: Learning through rewards and penalties\n",
    "\n",
    "Applications include image recognition, speech processing, and recommendation systems\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "for filepath,content in sample_texts.items():\n",
    "    with open(filepath,\"w\",encoding=\"utf-8\") as f:\n",
    "        f.write(content)\n",
    "print(\"Sample files created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7d605506",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Textloader\n",
    "#Structure raw text into Langchain Document format\n",
    "loader=TextLoader(\"../data/text_files/python_intro.txt\",encoding=\"utf-8\")\n",
    "document=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "721f036d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 4862.96it/s]\n"
     ]
    }
   ],
   "source": [
    "#Directory Loader\n",
    "#Load all text files from the directory \n",
    "dir_loader=DirectoryLoader(\n",
    "    \"../data/text_files\",\n",
    "    glob=\"**/*.txt\",\n",
    "    loader_cls=TextLoader,\n",
    "    loader_kwargs={\"encoding\":\"utf-8\"},\n",
    "    show_progress=True\n",
    ")\n",
    "documents=dir_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d0158ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.22it/s]\n"
     ]
    }
   ],
   "source": [
    "dir_loader=DirectoryLoader(\n",
    "    \"../data/pdf\",\n",
    "    glob=\"**/*.pdf\",\n",
    "    loader_cls=PyMuPDFLoader,\n",
    "    show_progress=True\n",
    ")\n",
    "pdf_documents=dir_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6b2de4d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 PDF files to process\n",
      "\n",
      "Processing: javanotes5.pdf\n",
      "  ✓ Loaded 699 pages\n",
      "\n",
      "Processing: Dsa.pdf\n",
      "  ✓ Loaded 112 pages\n",
      "\n",
      "Processing: 245078a9e1ffa18cdbd78bd9f50285d9.pdf\n",
      "  ✓ Loaded 17 pages\n",
      "\n",
      "Total documents loaded: 828\n"
     ]
    }
   ],
   "source": [
    "### Read all the pdf's inside the directory\n",
    "def process_all_pdfs(pdf_directory):\n",
    "    \"\"\"Process all PDF files in a directory\"\"\"\n",
    "    all_documents = []\n",
    "    pdf_dir = Path(pdf_directory)\n",
    "    \n",
    "    # Find all PDF files recursively\n",
    "    pdf_files = list(pdf_dir.glob(\"**/*.pdf\"))\n",
    "    \n",
    "    print(f\"Found {len(pdf_files)} PDF files to process\")\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"\\nProcessing: {pdf_file.name}\")\n",
    "        try:\n",
    "            loader = PyPDFLoader(str(pdf_file))\n",
    "            documents = loader.load()\n",
    "            \n",
    "            # Add source information to metadata\n",
    "            for doc in documents:\n",
    "                doc.metadata['source_file'] = pdf_file.name\n",
    "                doc.metadata['file_type'] = 'pdf'\n",
    "            \n",
    "            all_documents.extend(documents)\n",
    "            print(f\"  ✓ Loaded {len(documents)} pages\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error: {e}\")\n",
    "    \n",
    "    print(f\"\\nTotal documents loaded: {len(all_documents)}\")\n",
    "    return all_documents\n",
    "\n",
    "# Process all PDFs in the data directory\n",
    "all_pdf_documents = process_all_pdfs(\"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "534399f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 828 documents into 2916 chunks\n",
      "\n",
      "Example chunk:\n",
      "Content: Introduction to Programming Using Java\n",
      "V ersion 5.0, December 2006\n",
      "(Version 5.0.2, with minor corrections, November 2007)\n",
      "David J. Eck\n",
      "Hobart and William Smith Colleges...\n",
      "Metadata: {'producer': 'AFPL Ghostscript 8.51', 'creator': 'dvips(k) 5.95b Copyright 2005 Radical Eye Software', 'creationdate': 'D:20071115202101', 'moddate': 'D:20071115202101', 'title': 'javanotes.dvi', 'source': '../data/pdf/javanotes5.pdf', 'total_pages': 699, 'page': 0, 'page_label': '1', 'source_file': 'javanotes5.pdf', 'file_type': 'pdf'}\n"
     ]
    }
   ],
   "source": [
    "# Text splitting get into chunks\n",
    "def split_documents(documents,chunk_size=1000,chunk_overlap=200):\n",
    "    \"\"\"Split documents into smaller chunks for better RAG performance\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    split_docs = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(split_docs)} chunks\")\n",
    "    \n",
    "   \n",
    "    if split_docs:\n",
    "        print(f\"\\nExample chunk:\")\n",
    "        print(f\"Content: {split_docs[0].page_content[:200]}...\")\n",
    "        print(f\"Metadata: {split_docs[0].metadata}\")\n",
    "    \n",
    "    return split_docs\n",
    "chunks=split_documents(all_pdf_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "22153ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: all-MiniLM-L6-v2\n",
      "Model loaded sucessfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.EmbeddingManager at 0x756b33c87200>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Embeddings\n",
    "class EmbeddingManager:\n",
    "    def __init__(self,model_name:str=\"all-MiniLM-L6-v2\"):\n",
    "        self.model_name=model_name\n",
    "        self.model=None\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        try:\n",
    "            print(f\"Loading embedding model: {self.model_name}\")\n",
    "            self.model=SentenceTransformer(self.model_name)\n",
    "            print(f\"Model loaded sucessfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model {self.model_name}: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def generate_embeddings(self,texts: List[str]) -> np.ndarray:\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model not loaded\")\n",
    "        print(f\"Generating embeddings for {len(texts)} texts....\")\n",
    "        embeddings=self.model.encode(texts,show_progress_bar=True)\n",
    "        print(f\"Generated embeddings with shape: {embeddings.shape}\")\n",
    "        return embeddings\n",
    "    \n",
    "\n",
    "embedding_manager=EmbeddingManager()\n",
    "embedding_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "38ee5292",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.VectorStore at 0x756b33cdd7c0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#VectoreStore\n",
    "class VectorStore:\n",
    "    def __init__(self,collection_name: str=\"pdf_documents\",persist_directory:str=\"../data/vector_store\"):\n",
    "        self.collection_name=collection_name\n",
    "        self.persist_directory=persist_directory\n",
    "        self.client=None\n",
    "        self.collection=None\n",
    "        self._initialize_store()\n",
    "\n",
    "\n",
    "    def _initialize_store(self):\n",
    "        try:\n",
    "            os.makedirs(self.persist_directory,exist_ok=True)\n",
    "            self.client=chromadb.PersistentClient(path=self.persist_directory)\n",
    "            \n",
    "            self.collection=self.client.get_or_create_collection(\n",
    "\n",
    "                name=self.collection_name,\n",
    "                metadata={\"description\":\"PDF documents embeddings for RAG\"}\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "    def add_documents(self,documents:List[Any],embeddings:np.ndarray):\n",
    "        if len(documents)!=len(embeddings):\n",
    "            raise ValueError(\"Number of documents must match the embeddings\")\n",
    "        print(f\"Adding {len(document)} douments to vector store\")\n",
    "\n",
    "        ids=[]\n",
    "        metadatas=[]\n",
    "        documents_text=[]\n",
    "        embeddings_list=[]\n",
    "\n",
    "        for i, (doc,embedding) in enumerate(zip(documents,embeddings)):\n",
    "            doc_id=f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "            ids.append(doc_id)\n",
    "\n",
    "            metadata=dict(doc.metadata)\n",
    "            metadata['doc_index']=i\n",
    "            metadata['context_length']=len(doc.page_content)\n",
    "            metadatas.append(metadata)\n",
    "\n",
    "            documents_text.append(doc.page_content)\n",
    "            embeddings_list.append(embedding.tolist())\n",
    "\n",
    "\n",
    "        try:\n",
    "            self.collection.add(\n",
    "                ids=ids,\n",
    "                embeddings=embeddings_list,\n",
    "                metadatas=metadatas,\n",
    "                documents=documents_text\n",
    "            )\n",
    "            print(f'Sucessfully added {len(documents)} documents to vector store')\n",
    "            print(f'Total documents in collection: {self.collection.count()}')\n",
    "        except Exception as e:\n",
    "            print(f\"Error adding documents to vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "vectorstore=VectorStore()\n",
    "vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "58b79d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for 2916 texts....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 92/92 [01:21<00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (2916, 384)\n",
      "Adding 1 douments to vector store\n",
      "Sucessfully added 2916 documents to vector store\n",
      "Total documents in collection: 3147\n"
     ]
    }
   ],
   "source": [
    "#Converting texts to embeddings\n",
    "texts=[doc.page_content for doc in chunks]\n",
    "embeddings=embedding_manager.generate_embeddings(texts)\n",
    "\n",
    "vectorstore.add_documents(chunks,embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "76978ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retriever Pipeline from Vector Store\n",
    "class RAGRetriever:\n",
    "    def __init__(self,vector_store: VectorStore, embedding_manager: EmbeddingManager):\n",
    "        self.vector_store=vector_store\n",
    "        self.embedding_manager=embedding_manager\n",
    "         \n",
    "\n",
    "    def retrieve(self, query: str, top_k: int = 5, score_threshold: float = 0.0) -> List[Dict[str, Any]]:\n",
    "  \n",
    "        print(f\"Retrieving documents for query: '{query}'\")\n",
    "        print(f\"Top K: {top_k}, Score threshold: {score_threshold}\")\n",
    "        \n",
    "        query_embedding = self.embedding_manager.generate_embeddings([query])[0]\n",
    "        \n",
    "        try:\n",
    "            results = self.vector_store.collection.query(\n",
    "                query_embeddings=[query_embedding.tolist()],\n",
    "                n_results=top_k\n",
    "            )\n",
    "            retrieved_docs = []\n",
    "            \n",
    "            if results['documents'] and results['documents'][0]:\n",
    "                documents = results['documents'][0]\n",
    "                metadatas = results['metadatas'][0]\n",
    "                distances = results['distances'][0]\n",
    "                ids = results['ids'][0]\n",
    "                \n",
    "                for i, (doc_id, document, metadata, distance) in enumerate(zip(ids, documents, metadatas, distances)):\n",
    "\n",
    "                    similarity_score = 1 - distance\n",
    "                    \n",
    "                    if similarity_score >= score_threshold:\n",
    "                        retrieved_docs.append({\n",
    "                            'id': doc_id,\n",
    "                            'content': document,\n",
    "                            'metadata': metadata,\n",
    "                            'similarity_score': similarity_score,\n",
    "                            'distance': distance,\n",
    "                            'rank': i + 1\n",
    "                        })\n",
    "                \n",
    "                print(f\"Retrieved {len(retrieved_docs)} documents (after filtering)\")\n",
    "            else:\n",
    "                print(\"No documents found\")\n",
    "            \n",
    "            return retrieved_docs\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during retrieval: {e}\")\n",
    "            return []\n",
    "\n",
    "rag_retriever=RAGRetriever(vectorstore,embedding_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b707d03e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'What is Chemistry'\n",
      "Top K: 5, Score threshold: 0.0\n",
      "Generating embeddings for 1 texts....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 109.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 1 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 'doc_8c8d0daa_2869',\n",
       "  'content': 'chemistry and also realised that it influences every\\nsphere of human life.  The principles of chemistry have\\nbeen used for the benefit of mankind. Think of\\ncleanliness — the materials like soaps, detergents,\\nhousehold bleaches, tooth pastes, etc. will come to your\\nmind. Look towards the beautiful clothes — immediately\\nchemicals of the synthetic fibres used for making clothes\\nand chemicals giving colours to them will come to your\\nmind. Food materials — again a number of chemicals\\nabout which you have learnt in the previous Unit will\\nappear in your mind. Of course, sickness and diseases\\nremind us of medicines — again chemicals. Explosives,\\nfuels, rocket propellents, building and electronic\\nmaterials, etc., are all chemicals. Chemistry has\\ninfluenced our life so much that we do not even realise\\nthat we come across chemicals at every moment; that\\nwe ourselves are beautiful chemical creations and all\\nour activities are controlled by chemicals. In this Unit,',\n",
       "  'metadata': {'moddate': 'D:20120507200058',\n",
       "   'doc_index': 2869,\n",
       "   'total_pages': 17,\n",
       "   'page_label': '1',\n",
       "   'context_length': 966,\n",
       "   'author': 'surenderkumar',\n",
       "   'page': 0,\n",
       "   'file_type': 'pdf',\n",
       "   'title': 'Unit_16.pmd',\n",
       "   'producer': 'GPL Ghostscript 8.15',\n",
       "   'creationdate': 'D:20120507200058',\n",
       "   'source': '../data/pdf/245078a9e1ffa18cdbd78bd9f50285d9.pdf',\n",
       "   'creator': 'PageMaker 7.0',\n",
       "   'source_file': '245078a9e1ffa18cdbd78bd9f50285d9.pdf'},\n",
       "  'similarity_score': 0.058647215366363525,\n",
       "  'distance': 0.9413527846336365,\n",
       "  'rank': 1}]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriever.retrieve(\"What is Chemistry\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
